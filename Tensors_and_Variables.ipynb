{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPNZdbYM09xP2HTp1vMg/D7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrl7012005/Tensor-Flow-Computer-Vision-Course/blob/main/Tensors_and_Variables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EcNOUlYb2nU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tensors Introduction"
      ],
      "metadata": {
        "id": "I_xOVm7-cI4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_3D = tf.constant([\n",
        "\n",
        "    [[2,3,4.2],\n",
        "    [4.1,0.7,-3.6]],\n",
        "\n",
        "    [[0.1, 0.4, -5],\n",
        "    [6.2, 5.1, -0.3]],\n",
        "\n",
        "    [[-2.9, 0.7, 1.6],\n",
        "     [-9.7, 4.4, 2.2]]\n",
        "\n",
        "], dtype=tf.float32)\n",
        "\n",
        "print(tensor_3D)\n",
        "\n",
        "print(tensor_3D.shape)\n",
        "print(tensor_3D.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypDL25YQcgE5",
        "outputId": "17325869-6a4e-4269-e044-cb8e237337c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[ 2.   3.   4.2]\n",
            "  [ 4.1  0.7 -3.6]]\n",
            "\n",
            " [[ 0.1  0.4 -5. ]\n",
            "  [ 6.2  5.1 -0.3]]\n",
            "\n",
            " [[-2.9  0.7  1.6]\n",
            "  [-9.7  4.4  2.2]]], shape=(3, 2, 3), dtype=float32)\n",
            "(3, 2, 3)\n",
            "<dtype: 'float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declaring datatype in declaring tensor, brings an error if the dtype doesn't match the elements of the tensor. Thus  use casting method to change the data type. You don't have to declare dtype when declaring tensor as it will default. Its more of a formality."
      ],
      "metadata": {
        "id": "eE4S0aVgfA4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "casted_tensor_3D = tf.cast(tensor_3D, dtype=tf.int32)\n",
        "print(casted_tensor_3D)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXPhp5EXfNQB",
        "outputId": "345d51cb-2efa-42fc-a094-c9b44f761d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[ 2  3  4]\n",
            "  [ 4  0 -3]]\n",
            "\n",
            " [[ 0  0 -5]\n",
            "  [ 6  5  0]]\n",
            "\n",
            " [[-2  0  1]\n",
            "  [-9  4  2]]], shape=(3, 2, 3), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A numpy array can be converted to a tensor. Use the methods list as a guide: https://www.tensorflow.org/api_docs/python/tf\n"
      ],
      "metadata": {
        "id": "wkV1UJR6juNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np_array = np.array([1,2,3])\n",
        "tensor = tf.convert_to_tensor(np_array)\n",
        "\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIMRdtFMjxRF",
        "outputId": "a72cfecc-f580-4fd9-b34d-6c9a9a909e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 2 3], shape=(3,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "eye- Identity tensor\n",
        "\n",
        "Batch size can be changed into an array. E.g. [2, ] represents 2 batches of 2D tensors. [2, 4] represents a batch size of 2x4 giving a 4D tensor etc. Each batch is a 2D tensor with rows and columns indicated."
      ],
      "metadata": {
        "id": "3EPp0CqSk1wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "identity = tf.eye(5, num_columns=2, batch_shape=[2, 3], dtype=tf.dtypes.float32)\n",
        "print(identity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOh0N3O-k4O_",
        "outputId": "5e0757c1-908c-4fd8-d7a4-1d80374ad33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[1. 0.]\n",
            "   [0. 1.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]]\n",
            "\n",
            "  [[1. 0.]\n",
            "   [0. 1.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]]\n",
            "\n",
            "  [[1. 0.]\n",
            "   [0. 1.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0.]\n",
            "   [0. 1.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]]\n",
            "\n",
            "  [[1. 0.]\n",
            "   [0. 1.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]]\n",
            "\n",
            "  [[1. 0.]\n",
            "   [0. 1.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]]]], shape=(2, 3, 5, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default batch_size and num_columns are set to none. You can obtain a more specific tensors by filling all entries with a certain number. The first argument is the shape, the next is the number. Theres also a specific command for filling with 1, called tf.ones but now the 2nd argument is instead the data type (tf.dtypes.float32). You can fill with ones in the same shape as another matrix by just inputting that tensor into the argument. Same thing with tf.zeros and zeros_like.\n"
      ],
      "metadata": {
        "id": "auPBqsXwm-Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nine = tf.fill([1, 2, 4], 9)\n",
        "print(nine)\n",
        "\n",
        "one = tf.ones([1, 2, 4], tf.dtypes.float32)\n",
        "print(one)\n",
        "\n",
        "one2 = tf.ones_like(tensor_3D)\n",
        "print(one2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62Kx9CVknpbK",
        "outputId": "9eb5b379-3ab6-4de1-d2c3-7c4a6631c152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[9 9 9 9]\n",
            "  [9 9 9 9]]], shape=(1, 2, 4), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[[1. 1. 1. 1.]\n",
            "  [1. 1. 1. 1.]]], shape=(1, 2, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]]], shape=(3, 2, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can return the shape of the tensor as another tensor and rank (same as matrix rank). And size is the number of elements and you can output this number as a float or int using out_type."
      ],
      "metadata": {
        "id": "ucoKene9oYBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.shape(one))\n",
        "tf.rank(one2), tf.size(tensor_3D, out_type=tf.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw2XjjPwpMSe",
        "outputId": "931bf8b4-a24d-422b-8fcf-91eb699c7c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 2 4], shape=(3,), dtype=int32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=int32, numpy=3>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=18.0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a randomized tensor, using values of a normal distribution.\n",
        "\n",
        "first input is shape and every other argument is just automatically assumed but you can change it."
      ],
      "metadata": {
        "id": "xHbZPel4qKp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random = tf.random.normal(\n",
        "    [2, 4, 7],\n",
        "    mean=0.0,\n",
        "    stddev=1.0,\n",
        "    dtype=tf.dtypes.float32,\n",
        "    seed=None,\n",
        "    name=None\n",
        ")\n",
        "\n",
        "print(random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4VRYfxrqm5S",
        "outputId": "05072ad4-79c3-4e4b-f485-a4ad310e7901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[-0.7775584  -0.68732744 -0.17085238  1.0447042  -0.59543717\n",
            "   -2.7913213  -0.597939  ]\n",
            "  [ 1.2267044   0.30389786 -0.03362191  0.46171632  0.38719705\n",
            "   -0.13575655 -1.3939846 ]\n",
            "  [-0.3042363   1.15801    -0.24138272  1.7302349   0.08956157\n",
            "    0.2130865   0.3000863 ]\n",
            "  [ 1.8876345  -1.5173038  -0.79857    -0.15440458  0.13925028\n",
            "    0.51138484 -1.3744308 ]]\n",
            "\n",
            " [[-0.37431848 -0.35268104 -0.31708273 -0.28965625 -0.21326295\n",
            "    0.16679314 -0.03827912]\n",
            "  [ 0.94507027 -1.1871272   0.03427466  0.42867464 -0.27954018\n",
            "   -0.33859655 -1.2472597 ]\n",
            "  [-0.49081835 -0.9661414  -0.19394498 -0.29743454  1.1235945\n",
            "   -1.1080405   0.44359627]\n",
            "  [-1.2680994  -0.18571621 -0.32005948  0.13289146 -0.31654206\n",
            "    0.95404315  1.931665  ]]], shape=(2, 4, 7), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can draw from a uniform distribution as well. Max values is 1 by default for floats, but must be specified for ints."
      ],
      "metadata": {
        "id": "RAr7AB6Orl7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random2 = tf.random.uniform(\n",
        "    [2, 3, 5],\n",
        "    minval=0,\n",
        "    maxval=None,\n",
        "    dtype=tf.dtypes.float32,\n",
        "    seed=None,\n",
        "    name=None\n",
        ")\n",
        "\n",
        "print(random2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kodmUMJrlke",
        "outputId": "1e6b80bd-ae1a-402f-89a5-5a623b78a3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0.5528207  0.45572293 0.6075269  0.7266499  0.9324819 ]\n",
            "  [0.54221225 0.7929466  0.5007788  0.7522783  0.0449177 ]\n",
            "  [0.2254368  0.4577824  0.6932833  0.26149368 0.25897908]]\n",
            "\n",
            " [[0.30619228 0.31849003 0.65334845 0.27522838 0.38209307]\n",
            "  [0.708547   0.12088406 0.02671635 0.40003896 0.9132843 ]\n",
            "  [0.4354571  0.5746273  0.4508332  0.09758592 0.68951774]]], shape=(2, 3, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the elements of a tensor by index works similar to an array. Including the [0:4] to return first 4 elements, however the return data structure is a tensor. If do [1:6:2] it goes from index 1 to 6 by jumping 2 elements at a time. : extracts everything. ... extracts everything before/after. can use them interchangeably."
      ],
      "metadata": {
        "id": "2y9Sa72ctq37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(random2[:])\n",
        "print(random2[:, 2:, 1:3])\n",
        "print(random2[..., 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i3VJUqqvjTn",
        "outputId": "190548ac-7dac-41e8-e5c4-8218850ec783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0.5528207  0.45572293 0.6075269  0.7266499  0.9324819 ]\n",
            "  [0.54221225 0.7929466  0.5007788  0.7522783  0.0449177 ]\n",
            "  [0.2254368  0.4577824  0.6932833  0.26149368 0.25897908]]\n",
            "\n",
            " [[0.30619228 0.31849003 0.65334845 0.27522838 0.38209307]\n",
            "  [0.708547   0.12088406 0.02671635 0.40003896 0.9132843 ]\n",
            "  [0.4354571  0.5746273  0.4508332  0.09758592 0.68951774]]], shape=(2, 3, 5), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.4577824 0.6932833]]\n",
            "\n",
            " [[0.5746273 0.4508332]]], shape=(2, 1, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.45572293 0.7929466  0.4577824 ]\n",
            " [0.31849003 0.12088406 0.5746273 ]], shape=(2, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Math Operations of Tensors\n"
      ],
      "metadata": {
        "id": "w-VqausRcI6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can apply operations to the whole tensor using tf.math. Can also put complex numbers in a tensor using j."
      ],
      "metadata": {
        "id": "-XLSE4wS854D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = tf.random.normal(\n",
        "    [3, 2, 2, 2],\n",
        "    mean=0.0,\n",
        "    stddev=1.0,\n",
        "    dtype=tf.dtypes.float32\n",
        ")\n",
        "\n",
        "print(tensor)\n",
        "\n",
        "absolute_tensor = tf.abs(tensor)\n",
        "print(absolute_tensor)\n",
        "\n",
        "sqrt_tensor = tf.sqrt(tensor)\n",
        "print(sqrt_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skO9CKxM8-rU",
        "outputId": "df1920f1-8bac-4f4f-ac39-d6038441fb88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[-0.6673196   0.16152245]\n",
            "   [-1.3026185  -1.7902454 ]]\n",
            "\n",
            "  [[ 0.07767393 -0.6114198 ]\n",
            "   [-2.2539957   0.20156978]]]\n",
            "\n",
            "\n",
            " [[[-1.5473981  -0.12229446]\n",
            "   [ 0.24614793  0.38473842]]\n",
            "\n",
            "  [[ 0.86254454 -1.5674287 ]\n",
            "   [-0.43830547  0.2938043 ]]]\n",
            "\n",
            "\n",
            " [[[-0.25307792 -0.9201909 ]\n",
            "   [ 0.8942041   1.5418025 ]]\n",
            "\n",
            "  [[ 0.00244554 -0.7260444 ]\n",
            "   [-1.014719   -1.1934392 ]]]], shape=(3, 2, 2, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[0.6673196  0.16152245]\n",
            "   [1.3026185  1.7902454 ]]\n",
            "\n",
            "  [[0.07767393 0.6114198 ]\n",
            "   [2.2539957  0.20156978]]]\n",
            "\n",
            "\n",
            " [[[1.5473981  0.12229446]\n",
            "   [0.24614793 0.38473842]]\n",
            "\n",
            "  [[0.86254454 1.5674287 ]\n",
            "   [0.43830547 0.2938043 ]]]\n",
            "\n",
            "\n",
            " [[[0.25307792 0.9201909 ]\n",
            "   [0.8942041  1.5418025 ]]\n",
            "\n",
            "  [[0.00244554 0.7260444 ]\n",
            "   [1.014719   1.1934392 ]]]], shape=(3, 2, 2, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[       nan 0.40189856]\n",
            "   [       nan        nan]]\n",
            "\n",
            "  [[0.27870044        nan]\n",
            "   [       nan 0.44896522]]]\n",
            "\n",
            "\n",
            " [[[       nan        nan]\n",
            "   [0.49613297 0.6202729 ]]\n",
            "\n",
            "  [[0.92873275        nan]\n",
            "   [       nan 0.5420372 ]]]\n",
            "\n",
            "\n",
            " [[[       nan        nan]\n",
            "   [0.94562364 1.2416934 ]]\n",
            "\n",
            "  [[0.04945238        nan]\n",
            "   [       nan        nan]]]], shape=(3, 2, 2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_1 = tf.constant([5, 3, 2, 4, 6], dtype=tf.int32)\n",
        "x_2 = tf.constant([2, 1, 6, 0, -2], dtype=tf.int32)\n",
        "\n",
        "print(tf.add(x_1, x_2))\n",
        "print(tf.subtract(x_1, x_2))\n",
        "print(tf.multiply(x_1, x_2))\n",
        "print(tf.divide(x_1, x_2))\n",
        "\n",
        "#Note these are element wise operations only\n",
        "\n",
        "print(tf.math.divide_no_nan(x_1, x_2)) #Throws 0 if divide by 0 instead of infinity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLRtUgiK_IJ0",
        "outputId": "f9ffe812-c401-43ba-e415-243f14b281ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([7 4 8 4 4], shape=(5,), dtype=int32)\n",
            "tf.Tensor([ 3  2 -4  4  8], shape=(5,), dtype=int32)\n",
            "tf.Tensor([ 10   3  12   0 -12], shape=(5,), dtype=int32)\n",
            "tf.Tensor([ 2.5         3.          0.33333333         inf -3.        ], shape=(5,), dtype=float64)\n",
            "tf.Tensor([ 2.5         3.          0.33333333  0.         -3.        ], shape=(5,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However if the size is not the same, then you get weird answers but its still element wise operation."
      ],
      "metadata": {
        "id": "fPkQQl27AoVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = tf.constant([2])\n",
        "print(tf.add(x_1, x_2)) #Adds each element by 2 as treats x_2 as being stretched out\n",
        "print(tf.multiply(x_1, x_2)) #Multiplies each element by 2, same treatment as addition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfpvZ5WdAuf7",
        "outputId": "b0cc7928-2d56-48a3-e8eb-386025ffa127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([7 5 4 6 8], shape=(5,), dtype=int32)\n",
            "tf.Tensor([10  6  4  8 12], shape=(5,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = tf.constant([[5], [3], [7]]) #This is 3 different rows\n",
        "print(tf.add(x_1, x_2)) #So each row is stretched accordingly\n",
        "print(tf.multiply(x_1, x_2)) #Row 1 * 5, row 2 * 3 etc.\n",
        "\n",
        "#Stretching occurs until the dimensions match"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJPkHxWHBP1b",
        "outputId": "d08f1e37-0170-42d9-c5a9-7c2dd1c4be3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[10  8  7  9 11]\n",
            " [ 8  6  5  7  9]\n",
            " [12 10  9 11 13]], shape=(3, 5), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[25 15 10 20 30]\n",
            " [15  9  6 12 18]\n",
            " [35 21 14 28 42]], shape=(3, 5), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x_2 = tf.constant([7, 2])\n",
        "# print(tf.add(x_1, x_2))\n",
        "# # Now you get error because cannot stretch"
      ],
      "metadata": {
        "id": "o23svICgBm5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The maximum/minimum operations compares element by element and returns a new tensor with the maximum of each element. argmax, argmin gives the index of the maximum/minimum value of the whole tensor."
      ],
      "metadata": {
        "id": "1fN0oxFXB447"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = tf.random.normal([6])\n",
        "t2 = tf.random.normal([6])\n",
        "\n",
        "print(tf.maximum(t1, t2))\n",
        "print(tf.minimum(t1, t2))\n",
        "\n",
        "print(tf.argmax(t1))\n",
        "\n",
        "print(tf.argmax(t2, 0)) #zero is the axis (does it column wise)\n",
        "#1 does it row wise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Adu2tHLAB_hc",
        "outputId": "d0004a6a-088c-44f7-cb3b-3ca196c130b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([-0.41831997  0.41540504  0.4123476  -0.23639508  1.8316064   1.2520461 ], shape=(6,), dtype=float32)\n",
            "tf.Tensor([-0.70973396 -0.9812629  -0.96650076 -0.54233783 -0.5950352  -0.06841824], shape=(6,), dtype=float32)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can compare tensors, using math.equal and can raise powers element wise and can find many more functions in the website (library)."
      ],
      "metadata": {
        "id": "z34bnynlD_2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Algebra of Tensors"
      ],
      "metadata": {
        "id": "k9i6ppzLF4dE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix multiplication uses the linalg part. Use tf.linalg.matmul to multiply the tensors."
      ],
      "metadata": {
        "id": "TicjKzXuFd7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = tf.random.normal([2, 2])\n",
        "t2 = tf.random.normal([2, 2])\n",
        "\n",
        "t3 = tf.linalg.matmul(t1, t2, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False)\n",
        "#Can actually specify the transpose or adjoint to be true so u multiply aTb etc.\n",
        "#Multiplication should be compatible in all dimensions\n",
        "\n",
        "print(t1 @ t2) #Shortcut for multiplcation\n",
        "\n",
        "t3 = tf.transpose(t3) #This is how you transpose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdlITUZPFdHx",
        "outputId": "dee3ff75-ddac-47a2-f641-c498691c6176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[2.375517   4.530316  ]\n",
            " [0.72898245 1.1020086 ]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For 3D tensors, it's like the following\n",
        "\n",
        "\\begin{pmatrix}A & B & C\\end{pmatrix}\n",
        "\n",
        "multiplied with\n",
        "\n",
        "\\begin{pmatrix}D & E & F\\end{pmatrix}\n",
        "\n",
        "where A, B, C, D, E, F are matrices.\n",
        "\n",
        "Thus if the dimensions are [a, b, c] and [x, y, z] then [b, c] and [y, z] represent the matrices so they must be compatible (c = y). Then a and x represent the number of matrices in each tensor.\n",
        "\n",
        "You just multiply each entry with their corresponding entry when doing batch multiplication as a tensor is a store of information.\n",
        "\n",
        "  \\begin{pmatrix}A & B & C\\end{pmatrix}\n",
        "×\n",
        "\\begin{pmatrix}D & E & F\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix} AD & BE & CF\\end{pmatrix}\n",
        "\n",
        "Its only element wise multiplication (blockwise), not defined the same way as matrix multiplication with dot products of tensors."
      ],
      "metadata": {
        "id": "1mOFAtOVG-HV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor linear algebra operations are essentially just matrix operations on each subsection. For example linalg.inv gives the inverse of each matrix in the tensor as the matrices are invertible. linalg.det gives the det of each matrix in the tensor"
      ],
      "metadata": {
        "id": "YnpA2H9POQdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can also do another type of tensor multiplication. Einstein summation.\n",
        "\n",
        "C_ik = Σ_j A_{i,j} B_{j,k}\n",
        "\n",
        "Also noted as ij,jk->ik. This is basically the normal way of matrix multiplication.\n",
        "\n",
        "First argument is the equation with strings around it.\n",
        "Second. Then the inputs one by one"
      ],
      "metadata": {
        "id": "QtmPyJxoZyok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t3 = tf.einsum('ij,jk->ik', t1, t2)\n",
        "print(t3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hye3H0cMLM56",
        "outputId": "85df1143-42d6-406d-e90d-3663dd17bf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[2.375517   4.530316  ]\n",
            " [0.72898245 1.1020086 ]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But can include any kind of equation so you can define any kind of matrix multiplication. Can even make tensor multiplication like matrix multiplication instead of element wise. Its very useful if need to multiply matrices by its transpose etc, then can just tell the computer how to multiply entries instead of computing product and taking transpose. Faster."
      ],
      "metadata": {
        "id": "tattANcxbkPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transpose\n",
        "\n",
        "print(tf.einsum('ij->ji', t1)) #Dont even need an output equation\n",
        "\n",
        "trace = tf.einsum('ii', t1)\n",
        "print(trace)\n",
        "\n",
        "\n",
        "s = tf.random.normal(shape=[7,5,3])\n",
        "t = tf.random.normal(shape=[7,3,2])\n",
        "e = tf.einsum('bij,bjk->bik', s, t)\n",
        "#Element wise tensor multiplication\n",
        "\n",
        "print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xJQC7rpbrms",
        "outputId": "d9a20db0-d821-477a-87d7-b54211848ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-0.3045729  -0.5991395 ]\n",
            " [-2.528023   -0.40803173]], shape=(2, 2), dtype=float32)\n",
            "tf.Tensor(-0.71260464, shape=(), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[ 0.01481996 -0.6949215 ]\n",
            "  [ 1.1967901  -1.777607  ]\n",
            "  [ 0.13478458 -2.8568497 ]\n",
            "  [-0.24946666 -2.4244761 ]\n",
            "  [-0.86981887 -0.52838224]]\n",
            "\n",
            " [[ 0.65365493 -0.33113068]\n",
            "  [-0.5295948   0.7307354 ]\n",
            "  [-0.7127991   0.03078699]\n",
            "  [-2.322647    1.8715788 ]\n",
            "  [ 1.9996098  -1.936737  ]]\n",
            "\n",
            " [[-0.10779395 -0.45835787]\n",
            "  [ 1.0456823   0.59483796]\n",
            "  [ 1.8299899   0.75769603]\n",
            "  [-0.7568112  -0.36821872]\n",
            "  [-0.96130455 -0.03453024]]\n",
            "\n",
            " [[ 3.6588554   2.4139798 ]\n",
            "  [ 4.297014    4.1317816 ]\n",
            "  [-2.6158948   3.405108  ]\n",
            "  [-0.7433123   0.3625856 ]\n",
            "  [ 0.74624705 -0.17431287]]\n",
            "\n",
            " [[-3.918371    3.3144467 ]\n",
            "  [ 1.958302   -1.687257  ]\n",
            "  [ 3.8919334  -5.8102975 ]\n",
            "  [-0.48033383  0.2087076 ]\n",
            "  [-1.2562861   3.1900797 ]]\n",
            "\n",
            " [[-0.44323137  0.16122656]\n",
            "  [ 1.0656384  -1.8349859 ]\n",
            "  [ 0.60838205  0.1080319 ]\n",
            "  [ 0.18937957 -0.9180374 ]\n",
            "  [ 1.171059   -0.15114236]]\n",
            "\n",
            " [[ 0.17578894 -0.70329285]\n",
            "  [ 0.22692773 -1.7272954 ]\n",
            "  [-0.2543344   0.106213  ]\n",
            "  [-1.7524055  -2.145004  ]\n",
            "  [ 0.46951878  1.1105634 ]]], shape=(7, 5, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Band part- copies a tensor but sets everything outside a central band to zero.\n",
        "\n",
        "Takes input tensor as first argument, the lower number as the 2nd and the upper number as the 3rd.\n",
        "\n",
        "num_lower- the number of subdiagonals to keep in lower half (if negative keep entire lower triangle)\n",
        "num_upper- same for upper half (negative keeps entire lower triangle)"
      ],
      "metadata": {
        "id": "45v4S85-W6fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = tf.constant([[ 4,  1,  2, 3],\n",
        "  [-1,  0,  1, 2],\n",
        "  [-2, -1,  0, 1],\n",
        "  [-3, -2, -1, 0]])\n",
        "\n",
        "print(tf.linalg.band_part(tensor, 0, -1)) #Keep upper half\n",
        "print(tf.linalg.band_part(tensor, 0, 0)) #diagonal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRgChVr0Yqef",
        "outputId": "7f27863b-030f-4d34-801e-81b1bb9f3f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[4 1 2 3]\n",
            " [0 0 1 2]\n",
            " [0 0 0 1]\n",
            " [0 0 0 0]], shape=(4, 4), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[4 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]], shape=(4, 4), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Flow Functions and Types of Tensors"
      ],
      "metadata": {
        "id": "JVls7swweeIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expand and squeeze functions**\n",
        "\n",
        "\n",
        "\n",
        "*   tf.expand_dims\n",
        "\n",
        "tf.expand_dims changes the shape of the tensor by adding an extra dimension in the specified axis.\n",
        "\n",
        "Axis = 0 -> Outer dimension, axis = 1 --> second entry, axis=-1 --> innermost dimension.\n",
        "\n",
        "The elements are the exact same, all it is is a restructuring of the shape of the tensor.\n",
        "\n",
        "\n",
        "*   tf.squeeze\n",
        "\n",
        "tf.squeeze removes ALL dimensions of size 1 (essentially squeezing the array to remove redundancies)\n",
        "\n",
        "You can also specify specific axes to remove the dimension of size 1\n",
        "\n"
      ],
      "metadata": {
        "id": "J1dk90neEUwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = tf.random.normal(\n",
        "    [2, 3, 3],\n",
        "    mean = 0.0,\n",
        "    stddev = 1.0,\n",
        "    dtype = tf.dtypes.float32,\n",
        "    seed = None,\n",
        "    name = None\n",
        ")\n",
        "\n",
        "t1 = tf.expand_dims(t1, axis=-1)\n",
        "t1 = tf.squeeze(t1, axis=-1)\n",
        "\n",
        "print(t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5BiVJjqFn_3",
        "outputId": "460de777-1c1f-418c-8f87-1b2fc999b7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[-0.60632926  0.99273664 -1.0332196 ]\n",
            "  [ 0.14794634  0.8323076   1.1455222 ]\n",
            "  [ 0.6604629   0.98629373 -1.2989569 ]]\n",
            "\n",
            " [[-0.39160913 -1.1978298  -0.08076437]\n",
            "  [ 0.08145501  3.14344     0.95903295]\n",
            "  [ 0.10617587  0.30995613 -0.8638287 ]]], shape=(2, 3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reshape**\n",
        "\n",
        "tf.reshape returns the same tensor with the same values in the same order except just restructures it and puts brackets in different places. Has to be compatible with the original shape, i.e. new shapes have to divide original shape.\n",
        "\n",
        "Putting a negative 1 for the very last argument. For example if the desired shape is [3, 6] then putting [3, -1] will automatically fill the last segment as required."
      ],
      "metadata": {
        "id": "AoCEU2RxGdEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# t1 = tf.reshape(t1, [3, 6])\n",
        "# print(t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aymx2I9HAOb",
        "outputId": "d6f431fb-2a78-4a8b-91d3-760e1efcbf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-1.043106   -0.9625143  -0.4299462  -0.62110555  1.395191   -0.13434443]\n",
            " [-0.15482023 -0.7137331   1.4525734   1.0223064  -0.33131275 -0.7067358 ]\n",
            " [-1.0760058   0.12381363 -1.2108891   0.6864058  -0.06298596 -0.61188763]], shape=(3, 6), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concatenation**\n",
        "\n",
        "Given some tensors, you can concatenate the entries (provided compatibility) along certain dimensions.\n",
        "\n",
        "e.g. if you have t1, t2, t3\n",
        "\n",
        "then you can concatenate them along axis=0 which will just give a larger tensor with each of the tensors as an entry (concatenating them on the outer axis)\n",
        "etc.\n",
        "\n",
        "tf.concat([t1,t2,t3], 1)\n",
        "\n",
        "where 1 is the axis.\n",
        "\n",
        "If concatenating along a new axis, for example,\n",
        "\n",
        "tf.concat([tf.expand_dims(t, axis) for t in tensors], axis) then consider using a stack.\n",
        "\n",
        "**STACK**\n",
        "\n",
        "The above can be written as tf.stack(tensors, axis=axis) so if you have an array of tensors, you can concatenate all of them along a certain axis by using the stack function.\n",
        "\n",
        "If you have n tensors, putting them into an array and putting the array as an input to the stack function, will stack these tensors together (concatenate) but add an extra dimension of n wherever the axis is."
      ],
      "metadata": {
        "id": "uTtotbMgHRH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t2 = tf.random.normal([2, 3, 3])\n",
        "\n",
        "print(tf.concat([t1, t2], 0))\n",
        "print(tf.concat([t1, t2], 1))\n",
        "print(tf.concat([t1, t2], 2))\n",
        "\n",
        "print(tf.stack([t1, t2], 0))\n",
        "print(tf.stack([t1, t2], 1))\n",
        "print(tf.stack([t1, t2], 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6pb-AymKTiX",
        "outputId": "dd40524e-e8ce-4ccf-8138-dbc38fc404cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[-0.60632926  0.99273664 -1.0332196 ]\n",
            "  [ 0.14794634  0.8323076   1.1455222 ]\n",
            "  [ 0.6604629   0.98629373 -1.2989569 ]]\n",
            "\n",
            " [[-0.39160913 -1.1978298  -0.08076437]\n",
            "  [ 0.08145501  3.14344     0.95903295]\n",
            "  [ 0.10617587  0.30995613 -0.8638287 ]]\n",
            "\n",
            " [[-0.20576544  1.6690203   1.3018458 ]\n",
            "  [-0.9085631   0.33456197  1.2912111 ]\n",
            "  [-0.22119798  0.06077502 -0.60229415]]\n",
            "\n",
            " [[ 0.51765317 -0.05285897  0.91976064]\n",
            "  [-0.872626   -0.6540979   0.93636554]\n",
            "  [ 0.23878624 -1.2829777  -0.62534916]]], shape=(4, 3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-0.60632926  0.99273664 -1.0332196 ]\n",
            "  [ 0.14794634  0.8323076   1.1455222 ]\n",
            "  [ 0.6604629   0.98629373 -1.2989569 ]\n",
            "  [-0.20576544  1.6690203   1.3018458 ]\n",
            "  [-0.9085631   0.33456197  1.2912111 ]\n",
            "  [-0.22119798  0.06077502 -0.60229415]]\n",
            "\n",
            " [[-0.39160913 -1.1978298  -0.08076437]\n",
            "  [ 0.08145501  3.14344     0.95903295]\n",
            "  [ 0.10617587  0.30995613 -0.8638287 ]\n",
            "  [ 0.51765317 -0.05285897  0.91976064]\n",
            "  [-0.872626   -0.6540979   0.93636554]\n",
            "  [ 0.23878624 -1.2829777  -0.62534916]]], shape=(2, 6, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-0.60632926  0.99273664 -1.0332196  -0.20576544  1.6690203\n",
            "    1.3018458 ]\n",
            "  [ 0.14794634  0.8323076   1.1455222  -0.9085631   0.33456197\n",
            "    1.2912111 ]\n",
            "  [ 0.6604629   0.98629373 -1.2989569  -0.22119798  0.06077502\n",
            "   -0.60229415]]\n",
            "\n",
            " [[-0.39160913 -1.1978298  -0.08076437  0.51765317 -0.05285897\n",
            "    0.91976064]\n",
            "  [ 0.08145501  3.14344     0.95903295 -0.872626   -0.6540979\n",
            "    0.93636554]\n",
            "  [ 0.10617587  0.30995613 -0.8638287   0.23878624 -1.2829777\n",
            "   -0.62534916]]], shape=(2, 3, 6), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[-0.60632926  0.99273664 -1.0332196 ]\n",
            "   [ 0.14794634  0.8323076   1.1455222 ]\n",
            "   [ 0.6604629   0.98629373 -1.2989569 ]]\n",
            "\n",
            "  [[-0.39160913 -1.1978298  -0.08076437]\n",
            "   [ 0.08145501  3.14344     0.95903295]\n",
            "   [ 0.10617587  0.30995613 -0.8638287 ]]]\n",
            "\n",
            "\n",
            " [[[-0.20576544  1.6690203   1.3018458 ]\n",
            "   [-0.9085631   0.33456197  1.2912111 ]\n",
            "   [-0.22119798  0.06077502 -0.60229415]]\n",
            "\n",
            "  [[ 0.51765317 -0.05285897  0.91976064]\n",
            "   [-0.872626   -0.6540979   0.93636554]\n",
            "   [ 0.23878624 -1.2829777  -0.62534916]]]], shape=(2, 2, 3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[-0.60632926  0.99273664 -1.0332196 ]\n",
            "   [ 0.14794634  0.8323076   1.1455222 ]\n",
            "   [ 0.6604629   0.98629373 -1.2989569 ]]\n",
            "\n",
            "  [[-0.20576544  1.6690203   1.3018458 ]\n",
            "   [-0.9085631   0.33456197  1.2912111 ]\n",
            "   [-0.22119798  0.06077502 -0.60229415]]]\n",
            "\n",
            "\n",
            " [[[-0.39160913 -1.1978298  -0.08076437]\n",
            "   [ 0.08145501  3.14344     0.95903295]\n",
            "   [ 0.10617587  0.30995613 -0.8638287 ]]\n",
            "\n",
            "  [[ 0.51765317 -0.05285897  0.91976064]\n",
            "   [-0.872626   -0.6540979   0.93636554]\n",
            "   [ 0.23878624 -1.2829777  -0.62534916]]]], shape=(2, 2, 3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[-0.60632926  0.99273664 -1.0332196 ]\n",
            "   [-0.20576544  1.6690203   1.3018458 ]]\n",
            "\n",
            "  [[ 0.14794634  0.8323076   1.1455222 ]\n",
            "   [-0.9085631   0.33456197  1.2912111 ]]\n",
            "\n",
            "  [[ 0.6604629   0.98629373 -1.2989569 ]\n",
            "   [-0.22119798  0.06077502 -0.60229415]]]\n",
            "\n",
            "\n",
            " [[[-0.39160913 -1.1978298  -0.08076437]\n",
            "   [ 0.51765317 -0.05285897  0.91976064]]\n",
            "\n",
            "  [[ 0.08145501  3.14344     0.95903295]\n",
            "   [-0.872626   -0.6540979   0.93636554]]\n",
            "\n",
            "  [[ 0.10617587  0.30995613 -0.8638287 ]\n",
            "   [ 0.23878624 -1.2829777  -0.62534916]]]], shape=(2, 3, 2, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding**\n",
        "\n",
        "Padding a tensor does the following:\n",
        "\n",
        "Suppose a tensor has n dimensions (its shape has n elements)\n",
        "\n",
        "What padding does in each dimension of the tensor is to add a certain number of values before (in that dimension) and certain number of values after (in that dimension). The value will just be a tensor of that size full of numbers (like concatenation)\n",
        "\n",
        "Padding takes a 2x2 tensor with N elements. Each index corresponds to a dimension and the first element says how many values before and the second, how many values after.\n",
        "\n",
        "The value can be controlled by the constant_values=5 argument\n",
        "\n",
        "--THE MODE CAN ALSO BE CHANGED\n",
        "\n",
        "--Normally mode='CONSTANT'\n",
        "-Other modes are reflect which just reflects the whole tensor internally and symmetric which fills the values symmetrically (here the constant_value is ignored)"
      ],
      "metadata": {
        "id": "TmbQMeUvLhL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paddings = tf.constant([[6, 5], [4, 8], [9, 10]])\n",
        "\n",
        "print(tf.pad(t2, paddings, mode='CONSTANT', constant_values=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pifiSH1UO1bD",
        "outputId": "824ab414-3773-4c7f-807a-b7a40f5f5ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  ...\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]]\n",
            "\n",
            " [[-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  ...\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]]\n",
            "\n",
            " [[-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  ...\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  ...\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]]\n",
            "\n",
            " [[-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  ...\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]]\n",
            "\n",
            " [[-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  ...\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]\n",
            "  [-1. -1. -1. ... -1. -1. -1.]]], shape=(13, 15, 22), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GATHER FUNCTION**\n",
        "\n",
        "tf.gather takes a tensor, a set of indices and an axis. And returns another tensor with the elements dictated by the indices in a particular axis/dimension.\n",
        "\n",
        "tf.gather(params, indices, axis) by default axis is 0, and indices is usually a 1D array. If axis is 0, indices specifies which batches we should take of the tensor. If axis is 1, perhaps its rows, axis = 2 could be columns etc.\n",
        "\n",
        "indices could also be a 2D array or higher in which the shape of the new tensor will have the same outline as the shape of indices. Here indices specifies the index of certain elements in the tensor to be returned in this new shape.\n",
        "\n",
        "For example params = [A, B, C, D, E]\n",
        "\n",
        "indices = [[2, 3], [1, 2]] would yield [[C, D], [B, C]]\n",
        "\n",
        "If the index doesn't exist it will just fill it with zeros.\n",
        "\n",
        "-Theres also a batch dims argument, which essentially skips a dimension if batch_dims = 1 and 2 dimensions if its 2.\n",
        "\n",
        "batch_dims essentially aligns the batches with the indices (particularly if using gather nd, each mini tensor corresponds to a batch of the tensor)\n",
        "\n",
        "**Gather nd**\n",
        "\n",
        "Similar to gather but this time the index specifically specifies the shape so if the index is [[0, 2], [4, 1]] for example then the resulting tensor is the element in the 0, 2 position and the element in the 4,1 position. If the dimensions don't match then it doesn't specify a singular element but the entire dimension of elements."
      ],
      "metadata": {
        "id": "2q82q2qEP18N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "print(\"----------- \\n #1\")\n",
        "params = tf.constant(['p0', 'p1', 'p2', 'p3', 'p4', 'p5'])\n",
        "indices = [2, 0, 2, 5]\n",
        "print(tf.gather(params, indices).numpy())\n",
        "#Will return ['p2', 'p0', 'p2', 'p5']\n",
        "\n",
        "#2\n",
        "print(\"----------- \\n #2\")\n",
        "print(tf.gather(params, [[2, 0], [2, 5]]).numpy())\n",
        "#Will return [['p2', 'p0'], ['p2', 'p5']]\n",
        "\n",
        "#3\n",
        "print(\"----------- \\n #3\")\n",
        "params = tf.constant([[0, 1.0, 2.0],\n",
        "                      [10.0, 11.0, 12.0],\n",
        "                      [20.0, 21.0, 22.0],\n",
        "                      [30.0, 31.0, 32.0]])\n",
        "print(tf.gather(params, indices=[3,1]).numpy())\n",
        "\n",
        "#4\n",
        "print(\"----------- \\n #4\")\n",
        "print(tf.gather_nd(\n",
        "    indices=[[0, 0],\n",
        "             [1, 1]],\n",
        "    params = [['a', 'b'],\n",
        "              ['c', 'd']]).numpy())\n",
        "\n",
        "#5\n",
        "print(\"----------- \\n #5\")\n",
        "print(tf.gather_nd(\n",
        "    indices = [[1],\n",
        "               [0]],\n",
        "    params = [['a', 'b', 'c'],\n",
        "              ['d', 'e', 'f']]).numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKGDfbTuXTqm",
        "outputId": "ecea225f-70e1-461e-84c5-3ed113d201b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- \n",
            " #1\n",
            "[b'p2' b'p0' b'p2' b'p5']\n",
            "----------- \n",
            " #2\n",
            "[[b'p2' b'p0']\n",
            " [b'p2' b'p5']]\n",
            "----------- \n",
            " #3\n",
            "[[30. 31. 32.]\n",
            " [10. 11. 12.]]\n",
            "----------- \n",
            " #4\n",
            "[b'a' b'd']\n",
            "----------- \n",
            " #5\n",
            "[[b'd' b'e' b'f']\n",
            " [b'a' b'b' b'c']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAGGED TENSORS**\n",
        "\n",
        "Sometimes we may have data with not the same number of columns/rows/dimensions throughout the tensor (i.e. not rectangular). Then we cannot create a regular tensor but what we can do is create a ragged tensor which is like a non-rectangular tensor that preserves the weird shape."
      ],
      "metadata": {
        "id": "2eKaL7LxbvfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = [[1, 2, 3],\n",
        "          [3],\n",
        "          [1, 7],\n",
        "          [2, 1, 4]]\n",
        "\n",
        "print(tf.ragged.constant(tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d02S6cA1cBng",
        "outputId": "a4e7e003-7523-47b7-cd12-840f811faf62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[1, 2, 3], [3], [1, 7], [2, 1, 4]]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can also create a ragged tensor from a regular tensor by using boolean mask. This is another tensor where you put true in the exact positions you want in the ragged tensor and false otherwise."
      ],
      "metadata": {
        "id": "y8yB7_yCcmCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = [[2, 4, 6],\n",
        "          [1, 3, 5]]\n",
        "mask = [[True, False, False], [True, True, False]]\n",
        "print(tf.ragged.boolean_mask(tensor, mask))\n",
        "#Not including ragged still works but wont produce a ragged tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2pm4IItckXw",
        "outputId": "f8978a26-956d-488f-e3ea-a22948928010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[2], [1, 3]]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other ways to create a ragged tensor are from:\n",
        "\n",
        "1. tf.RaggedTensor.from_row_splits\n",
        "-Start with an array of values\n",
        "-specify an array called row_splits which specifies the indices we should split the rows\n",
        "\n",
        "For example row_splits=[0, 4, 4, 7, 8, 8] specifies that the first element will contain indices 0 up to but not including 4. Then from 4 to 4 which will be empty. Then from 4 up to 7. Then from 7 to 8, and finally 8 to 8 which is empty.\n",
        "\n",
        "2. tf.RaggedTensor.from_row_limits\n",
        "\n",
        "row_lengths=[4, 4, 7, 8, 8]\n",
        "\n",
        "Works very similarly as above except now they're limits so first one goes up to 4. Then from 4 to 4 then from 4 up to 7 etc.\n",
        "\n",
        "3. tf.RaggedTensor.from_row_lengths\n",
        "\n",
        "row_lengths=[4, 0, 3, 1, 0]\n",
        "\n",
        "Now the first element is of length 4, the next of length 0, etc."
      ],
      "metadata": {
        "id": "2CfGT3o5dsBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.RaggedTensor.from_row_splits(\n",
        "      values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
        "      row_splits=[0, 4, 4, 7, 8, 8]))\n",
        "\n",
        "print(tf.RaggedTensor.from_row_lengths(\n",
        "    values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
        "    row_lengths=[4, 0, 3, 1, 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShAw6I4ce-ca",
        "outputId": "e237b522-6311-4d5e-b9e8-bdf1964e9031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n",
            "<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPARSE TENSORS**\n",
        "\n",
        "Alot of the time, we will have data containing many zeros and its more efficient to use sparse tensors as they're optimized when there are zeros to skip calculation.\n",
        "\n",
        "indices specifies the indices containing non-zero values (each element is an index). Values give you the corresponding values at each index.\n",
        "\n",
        "dense_shape is the tensor shape."
      ],
      "metadata": {
        "id": "i5ENzk4bfvEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_sparse = tf.sparse.SparseTensor(indices = [[1,1], [3, 4]], values = [11, 56], dense_shape = [5,6])\n",
        "print(tensor_sparse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU7w2ZeGgP7v",
        "outputId": "27c238b1-7901-4223-957b-3ea2c5cea6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparseTensor(indices=tf.Tensor(\n",
            "[[1 1]\n",
            " [3 4]], shape=(2, 2), dtype=int64), values=tf.Tensor([11 56], shape=(2,), dtype=int32), dense_shape=tf.Tensor([5 6], shape=(2,), dtype=int64))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STRING TENSORS**\n",
        "\n",
        "Special methods to deal with strings"
      ],
      "metadata": {
        "id": "WcKE0Vg0h0cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_string = tf.constant([\"H\", \"A\", \"P\"])\n",
        "print(tf.strings.join(tensor_string, separator=\"^\")) #joins all the string\n",
        "#Many more methods"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn_g2fzGh21h",
        "outputId": "72c711e8-19be-431c-a5d4-2026fc80c681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'H^A^P', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VARIABLES**\n",
        "\n",
        "Our tensor values will need to be updated throughout so use variable instead of constant to represent tensors. Most tensor operations work on variables except reshaping. Pass in any tensor into the variable argument. The argument consists of a lot of hidden features e.g. trainable=None which says if it can be modified during training.\n",
        "\n",
        "You can assign other values to tensors without changing shape. assign_add and assign_sub adds to another tensor elementwise\n",
        "\n",
        "Variables are backed by tensors so operations on them are done on the backing tensors. Duplicating them creates a copy of the tensor, they will not share the same memory thus duplicated variables are not equal."
      ],
      "metadata": {
        "id": "5uDMIK22i3aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable([2, 3, 5])\n",
        "x.assign([1, 4, 7])\n",
        "print(x)\n",
        "x.assign_add([1, 3, 5])\n",
        "print(x)\n",
        "x.assign_sub([2, 2, 0])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkCrU9vwjE-X",
        "outputId": "1cbfad18-fd9e-4c03-ac8d-de858c779503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([1, 4, 7], dtype=int32)>\n",
            "<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([ 2,  7, 12], dtype=int32)>\n",
            "<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([ 0,  5, 12], dtype=int32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also choose which device you want your variable to be in (cpu, gpu, tpu) to check which one you're using in colab, click runtime and change runtime type.\n",
        "\n",
        "To be in a specific device do the following:"
      ],
      "metadata": {
        "id": "NseNMsCyl6P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('CPU:0'): #Replace CPU with GPU if u want\n",
        "  x = tf.variable(0.2)\n",
        "\n",
        "print(x.device) #Gives the device"
      ],
      "metadata": {
        "id": "lcElyInimYf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "8e42420c-2709-4002-f796-675ac1a2edc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'variable'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-446c8c12e85a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Replace CPU with GPU if u want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Gives the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'variable'"
          ]
        }
      ]
    }
  ]
}